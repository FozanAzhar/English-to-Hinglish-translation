{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j47jyBetoyTW",
        "outputId": "5e3f9912-6eae-4e92-c1cb-ffb0a67fda07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install sentencepiece\n",
        "\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFcKZj6no8yO",
        "outputId": "491e1dc4-163d-4494-814c-bdc663643d72"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def spot_nouns_verbs_custom(en_sentence):\n",
        "    # Tokenize the input English sentence into words\n",
        "    words = word_tokenize(en_sentence)\n",
        "\n",
        "    # Perform part-of-speech tagging on the words\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Define a list of auxiliary verbs in English\n",
        "    auxiliary_verbs = ['am', 'is', 'are', 'was', 'were', 'has', 'had']\n",
        "\n",
        "    # Extract nouns and verbs from the tagged words\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    verbs = [word for word, pos in tagged_words if pos.startswith('VB') and word not in auxiliary_verbs]\n",
        "\n",
        "    # Lemmatize verbs to bring them to their base form\n",
        "    verbs = [lemmatizer.lemmatize(verb, pos='v') for verb in verbs]\n",
        "\n",
        "    # Initialize a dictionary to store English and Hinglish translations\n",
        "    translated_words = {\n",
        "        'feedback': 'feedback',\n",
        "        'definitely': 'निश्चितरूप ',  # Translate 'definitely' to Hinglish\n",
        "        'section': 'section'  # Translate 'section' to Hinglish\n",
        "    }\n",
        "\n",
        "    # Translate nouns and verbs and add them to the dictionary\n",
        "    for noun in nouns:\n",
        "        hi_noun = hin_translation(noun)\n",
        "        translated_words[noun] = hi_noun\n",
        "\n",
        "    for verb in verbs:\n",
        "        hi_verb = hin_translation(verb)\n",
        "        # Take the first part of the Hinglish translation (removes extra details)\n",
        "        modified_value = hi_verb.split(' ', 1)[0]\n",
        "        translated_words[verb] = modified_value\n",
        "\n",
        "    return translated_words"
      ],
      "metadata": {
        "id": "zFegO1AeqD0E"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hin_translation(en_sentence):\n",
        "    # Encode the English sentence using the Hinglish model\n",
        "    inputs = tokenizer.encode(en_sentence, return_tensors=\"pt\")\n",
        "    translated_id = model.generate(inputs, max_length=150, num_return_sequences=1, num_beams=4)\n",
        "    # Decode the generated Hinglish text and handle ZWJ characters\n",
        "    translated_output = tokenizer.decode(translated_id[0], skip_special_tokens=True)\n",
        "    translated_output = translated_output.replace('\\u200d', '')  # Handling ZWJ characters\n",
        "    return translated_output"
      ],
      "metadata": {
        "id": "aBFf6f_nqJ-s"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_hinglish_custom(en_sentence):\n",
        "    # Get nouns and verbs translations\n",
        "    nouns = spot_nouns_verbs_custom(en_sentence)\n",
        "\n",
        "    # Translate the entire English sentence to Hinglish\n",
        "    hi_text = hin_translation(en_sentence)\n",
        "\n",
        "    # Replace translated nouns and verbs in the Hinglish text\n",
        "    for eng_word, hin_word in nouns.items():\n",
        "        hi_text = hi_text.replace(hin_word, eng_word)\n",
        "\n",
        "    return hi_text"
      ],
      "metadata": {
        "id": "0oLXkSkjtD7V"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switching Hindi nouns to English nouns to keep certain words in English\n",
        "def noun_switch(nouns, hinglish_text):\n",
        "    for key, value in nouns.items():\n",
        "        matches = re.findall(r'\\b' + re.escape(value) + r'\\b', hinglish_text)\n",
        "        for match in matches:\n",
        "            hinglish_text = hinglish_text.replace(match, key)\n",
        "    return hinglish_text"
      ],
      "metadata": {
        "id": "r-153lj80DWF"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences\n",
        "en_sentences = [\n",
        "    \"1. Definitely share your feedback in the comment section.\",\n",
        "    \"2. So even if it's a big video, I will clearly mention all the products.\",\n",
        "    \"3. I was waiting for my bag.\"\n",
        "]\n",
        "\n",
        "for en_sentence in en_sentences:\n",
        "    # Translate each English sentence to Hinglish and print the results\n",
        "    hinglish_translation = translate_to_hinglish_custom(en_sentence)\n",
        "    print(f\"English: {en_sentence}\")\n",
        "    print(f\"Hinglish: {hinglish_translation}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwWIWSJLtGdh",
        "outputId": "259f2ea9-adf4-443e-ca2a-bb8f6eee49ee"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: 1. Definitely share your feedback in the comment section.\n",
            "Hinglish: 1 निश्चित रूप से comment खण्ड में आपकी feedback share करें.\n",
            "\n",
            "English: 2. So even if it's a big video, I will clearly mention all the products.\n",
            "Hinglish: 2 अगर यह एक बड़ा video है, तो भी मैं स्पष्ट रूप से सभी productsों का mention करेंगे।\n",
            "\n",
            "English: 3. I was waiting for my bag.\n",
            "Hinglish: 3 मैं अपने बैग के लिए wait कर रहा था.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "no7PzgoltJZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}