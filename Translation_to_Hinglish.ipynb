{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j47jyBetoyTW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install sentencepiece\n",
        "\n",
        "# Load the pre-trained model for English to Hindi translation\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "YFcKZj6no8yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to translate English text to Hinglish"
      ],
      "metadata": {
        "id": "RPYPby1Df7oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spot_nouns_verbs_custom(en_sentence):\n",
        "    # Tokenize the input English sentence into words\n",
        "    words = word_tokenize(en_sentence)\n",
        "\n",
        "    # Perform part-of-speech tagging on the words\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Define a list of auxiliary verbs in English\n",
        "    auxiliary_verbs = ['am', 'is', 'are', 'was', 'were', 'has', 'had']\n",
        "\n",
        "    # Extract nouns and verbs from the tagged words\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    verbs = [word for word, pos in tagged_words if pos.startswith('VB') and word not in auxiliary_verbs]\n",
        "\n",
        "    # Lemmatize verbs to bring them to their base form\n",
        "    verbs = [lemmatizer.lemmatize(verb, pos='v') for verb in verbs]\n",
        "\n",
        "    # Initialize a dictionary to store English and Hinglish translations\n",
        "    translated_words = {\n",
        "        'feedback': 'feedback',\n",
        "        'definitely': 'निश्चितरूप ',  # Translate 'definitely' to Hinglish\n",
        "        'section': 'खण्ड'  # Translate 'section' to Hinglish\n",
        "    }\n",
        "\n",
        "    # Translate nouns and verbs and add them to the dictionary\n",
        "    for noun in nouns:\n",
        "        hin_noun = hin_translation(noun)\n",
        "        translated_words[noun] = hin_noun\n",
        "\n",
        "    for verb in verbs:\n",
        "        hin_verb = hin_translation(verb)\n",
        "        # Take the first part of the Hinglish translation (removes extra details)\n",
        "        modified_value = hin_verb.split(' ', 1)[0]\n",
        "        translated_words[verb] = modified_value\n",
        "\n",
        "    return translated_words"
      ],
      "metadata": {
        "id": "zFegO1AeqD0E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to translate English text to Hindi"
      ],
      "metadata": {
        "id": "39k5dIUhf_ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hin_translation(en_sentence):\n",
        "    # Encode the English sentence using the Hinglish model\n",
        "    inputs = tokenizer.encode(en_sentence, return_tensors=\"pt\")\n",
        "    translated_id = model.generate(inputs, max_length=150, num_return_sequences=1, num_beams=4)\n",
        "    # Decode the generated Hinglish text and handle ZWJ characters\n",
        "    translated_output = tokenizer.decode(translated_id[0], skip_special_tokens=True)\n",
        "    translated_output = translated_output.replace('\\u200d', '')  # Handling ZWJ characters\n",
        "    return translated_output"
      ],
      "metadata": {
        "id": "aBFf6f_nqJ-s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_hinglish_custom(en_sentence):\n",
        "    # Get nouns and verbs translations\n",
        "    nouns = spot_nouns_verbs_custom(en_sentence)\n",
        "\n",
        "    # Translate the entire English sentence to Hinglish\n",
        "    hin_text = hin_translation(en_sentence)\n",
        "\n",
        "    # Replace translated nouns and verbs in the Hinglish text\n",
        "    for eng_word, hin_word in nouns.items():\n",
        "        hin_text = hin_text.replace(hin_word, eng_word)\n",
        "\n",
        "    return hin_text"
      ],
      "metadata": {
        "id": "0oLXkSkjtD7V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switching Hindi nouns to English nouns to keep certain words in English\n",
        "def noun_switch(nouns, hinglish_text):\n",
        "    for key, value in nouns.items():\n",
        "        matches = re.findall(r'\\b' + re.escape(value) + r'\\b', hinglish_text)\n",
        "        for match in matches:\n",
        "            hinglish_text = hinglish_text.replace(match, key)\n",
        "    return hinglish_text"
      ],
      "metadata": {
        "id": "r-153lj80DWF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences\n",
        "en_sentences = [\n",
        "    \"1. Definitely share your feedback in the comment section.\",\n",
        "    \"2. So even if it's a big video, I will clearly mention all the products.\",\n",
        "    \"3. I was waiting for my bag.\",\n",
        "]\n",
        "\n",
        "for en_sentence in en_sentences:\n",
        "    # Translate each English sentence to Hinglish and print the results\n",
        "    hinglish_translation = translate_to_hinglish_custom(en_sentence)\n",
        "    print(f\"English: {en_sentence}\")\n",
        "    print(f\"Hinglish: {hinglish_translation}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwWIWSJLtGdh",
        "outputId": "8ee645c5-9219-40bd-852b-86dec4661a67"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: 1. Definitely share your feedback in the comment section.\n",
            "Hinglish: 1 निश्चित रूप से comment खण्ड में आपकी feedback share करें.\n",
            "\n",
            "English: 2. So even if it's a big video, I will clearly mention all the products.\n",
            "Hinglish: 2 अगर यह एक बड़ा video है, तो भी मैं स्पष्ट रूप से सभी productsों का mention करेंगे।\n",
            "\n",
            "English: 3. I was waiting for my bag.\n",
            "Hinglish: 3 मैं अपने बैग के लिए wait कर रहा था.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}